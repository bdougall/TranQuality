{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/w266/project/bleu_test/testing\n"
     ]
    }
   ],
   "source": [
    "# create a seperate folder to store everything\n",
    "!mkdir testing\n",
    "%cd testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'indicTrans'...\n",
      "remote: Enumerating objects: 509, done.\u001b[K\n",
      "remote: Counting objects: 100% (212/212), done.\u001b[K\n",
      "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
      "remote: Total 509 (delta 167), reused 132 (delta 118), pack-reused 297\u001b[K\n",
      "Receiving objects: 100% (509/509), 1.49 MiB | 7.59 MiB/s, done.\n",
      "Resolving deltas: 100% (294/294), done.\n",
      "/w266/project/bleu_test/testing/indicTrans\n",
      "Cloning into 'indic_nlp_library'...\n",
      "remote: Enumerating objects: 1325, done.\u001b[K\n",
      "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
      "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
      "remote: Total 1325 (delta 84), reused 89 (delta 41), pack-reused 1178\u001b[K\n",
      "Receiving objects: 100% (1325/1325), 9.57 MiB | 17.72 MiB/s, done.\n",
      "Resolving deltas: 100% (688/688), done.\n",
      "Cloning into 'indic_nlp_resources'...\n",
      "remote: Enumerating objects: 139, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
      "Receiving objects: 100% (139/139), 149.77 MiB | 26.52 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "Cloning into 'subword-nmt'...\n",
      "remote: Enumerating objects: 580, done.\u001b[K\n",
      "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 580 (delta 0), reused 1 (delta 0), pack-reused 576\u001b[K\n",
      "Receiving objects: 100% (580/580), 237.41 KiB | 2.70 MiB/s, done.\n",
      "Resolving deltas: 100% (349/349), done.\n",
      "/w266/project/bleu_test/testing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# clone the repo for running evaluation\n",
    "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
    "%cd indicTrans\n",
    "# clone requirements repositories\n",
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
    "!git clone https://github.com/rsennrich/subword-nmt.git\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 26.7 MB/s eta 0:00:01     |████████████████████████▌       | 7.3 MB 26.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mock\n",
      "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboardX\n",
      "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 44.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow\n",
      "  Downloading pyarrow-6.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.5 MB 46.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting indic-nlp-library\n",
      "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 7.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2021.10.23-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "\u001b[K     |████████████████████████████████| 748 kB 29.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 34.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n",
      "Collecting click\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.2)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.17.3)\n",
      "Collecting morfessor\n",
      "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Collecting sphinx-rtd-theme\n",
      "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinx-argparse\n",
      "  Downloading sphinx_argparse-0.3.1-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from click->sacremoses) (4.6.3)\n",
      "Collecting docutils<0.18\n",
      "  Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
      "\u001b[K     |████████████████████████████████| 575 kB 27.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinx>=1.6\n",
      "  Downloading Sphinx-4.2.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 22.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->click->sacremoses) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->click->sacremoses) (3.7.4.3)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.5\n",
      "  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sphinxcontrib-applehelp\n",
      "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 39.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (21.0)\n",
      "Collecting imagesize\n",
      "  Downloading imagesize-1.2.0-py2.py3-none-any.whl (4.8 kB)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (2.9.0)\n",
      "Collecting babel>=1.3\n",
      "  Downloading Babel-2.9.1-py2.py3-none-any.whl (8.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.8 MB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (57.4.0)\n",
      "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (2.26.0)\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting snowballstemmer>=1.1\n",
      "  Downloading snowballstemmer-2.1.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 3.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sphinxcontrib-htmlhelp>=2.0.0\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 13.3 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (3.0.1)\n",
      "Collecting alabaster<0.8,>=0.7\n",
      "  Downloading alabaster-0.7.12-py2.py3-none-any.whl (14 kB)\n",
      "Collecting sphinxcontrib-devhelp\n",
      "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 5.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sphinxcontrib-qthelp\n",
      "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.0->sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.0->sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests>=2.5.0->sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (2.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from requests>=2.5.0->sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx>=1.6->sphinx-rtd-theme->indic-nlp-library) (2.0.1)\n",
      "Installing collected packages: tqdm, regex, joblib, click, sacremoses, pytz, pandas, mock, colorama, portalocker, tabulate, sacrebleu, tensorboardX, pyarrow, morfessor, docutils, sphinxcontrib-serializinghtml, sphinxcontrib-applehelp, imagesize, babel, sphinxcontrib-jsmath, snowballstemmer, sphinxcontrib-htmlhelp, alabaster, sphinxcontrib-devhelp, sphinxcontrib-qthelp, sphinx, sphinx-rtd-theme, sphinx-argparse, indic-nlp-library\n",
      "Successfully installed alabaster-0.7.12 babel-2.9.1 click-8.0.3 colorama-0.4.4 docutils-0.17.1 imagesize-1.2.0 indic-nlp-library-0.81 joblib-1.1.0 mock-4.0.3 morfessor-2.0.6 pandas-1.1.5 portalocker-2.3.2 pyarrow-6.0.0 pytz-2021.3 regex-2021.10.23 sacrebleu-2.0.0 sacremoses-0.0.46 snowballstemmer-2.1.0 sphinx-4.2.0 sphinx-argparse-0.3.1 sphinx-rtd-theme-1.0.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5 tabulate-0.8.9 tensorboardX-2.4 tqdm-4.62.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 29804, done.\u001b[K\n",
      "remote: Counting objects: 100% (885/885), done.\u001b[K\n",
      "remote: Compressing objects: 100% (542/542), done.\u001b[K\n",
      "remote: Total 29804 (delta 395), reused 723 (delta 327), pack-reused 28919\u001b[K\n",
      "Receiving objects: 100% (29804/29804), 13.69 MiB | 6.92 MiB/s, done.\n",
      "Resolving deltas: 100% (22122/22122), done.\n",
      "/w266/project/bleu_test/testing/indicTrans/fairseq\n",
      "Obtaining file:///w266/project/bleu_test/testing/indicTrans/fairseq\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cython\n",
      "  Using cached Cython-0.29.24-cp36-cp36m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+bba000d) (2.0.0)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+bba000d) (1.14.6)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+bba000d) (0.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+bba000d) (4.62.3)\n",
      "Collecting torchaudio>=0.8.0\n",
      "  Downloading torchaudio-0.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bitarray\n",
      "  Downloading bitarray-2.3.4.tar.gz (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 8.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.20.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+bba000d) (1.19.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq==1.0.0a0+bba000d) (2021.10.23)\n",
      "Collecting omegaconf<2.1\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 1.4 kB/s  eta 0:00:01    |██▏                             | 61.2 MB 24.7 MB/s eta 0:00:34     |███████████████▊                | 433.4 MB 20.6 MB/s eta 0:00:22     |███████████████████▉            | 547.0 MB 43.5 MB/s eta 0:00:08     |██████████████████████████████▊ | 847.5 MB 40.7 MB/s eta 0:00:01     |███████████████████████████████ | 855.2 MB 40.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n",
      "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 24.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+bba000d) (0.8.9)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+bba000d) (2.3.2)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+bba000d) (0.4.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==1.0.0a0+bba000d) (2.20)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+bba000d) (3.7.4.3)\n",
      "Collecting PyYAML>=5.1.*\n",
      "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "\u001b[K     |████████████████████████████████| 603 kB 28.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.3.0-py3-none-any.whl (28 kB)\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 38.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+bba000d) (3.5.0)\n",
      "Building wheels for collected packages: bitarray, antlr4-python3-runtime\n",
      "  Building wheel for bitarray (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bitarray: filename=bitarray-2.3.4-cp36-cp36m-linux_x86_64.whl size=169870 sha256=c2d3b49345a88e67f4f884f0b479b5ee7cd6987a4a05a02c10af4e7b97722e42\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/b1/7e/9ddd1aab6da0bf7136e4f915c5b7f6b60ac738ac6122a1ced9\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=54f89270723cd8e3d1ed953fc94b6553e3e94669af95cc649a12d12490525eb2\n",
      "  Stored in directory: /root/.cache/pip/wheels/a8/04/35/9449686f1c26ff16f6224dc942e108329f3782185802ec6b93\n",
      "Successfully built bitarray antlr4-python3-runtime\n",
      "Installing collected packages: cython, torch, torchaudio, bitarray, PyYAML, omegaconf, importlib-resources, antlr4-python3-runtime, hydra-core, fairseq\n",
      "  Running setup.py develop for fairseq\n",
      "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.3.4 cython-0.29.24 fairseq hydra-core-1.0.7 importlib-resources-5.3.0 omegaconf-2.0.6 torch-1.10.0 torchaudio-0.10.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "/w266/project/bleu_test/testing/indicTrans\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
    "# Install fairseq from source\n",
    "!git clone https://github.com/pytorch/fairseq.git\n",
    "%cd fairseq\n",
    "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
    "!pip install --editable ./\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-30 03:26:20--  https://storage.googleapis.com/samanantar-public/V0.2/models/indic-en.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.185.128, 108.177.122.128, 74.125.138.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.185.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4551079075 (4.2G) [application/zip]\n",
      "Saving to: ‘indic-en.zip’\n",
      "\n",
      "indic-en.zip        100%[===================>]   4.24G  26.5MB/s    in 2m 43s  \n",
      "\n",
      "2021-10-30 03:29:03 (26.6 MB/s) - ‘indic-en.zip’ saved [4551079075/4551079075]\n",
      "\n",
      "Archive:  indic-en.zip\n",
      "   creating: indic-en/\n",
      "   creating: indic-en/vocab/\n",
      "  inflating: indic-en/vocab/bpe_codes.32k.SRC  \n",
      "  inflating: indic-en/vocab/vocab.SRC  \n",
      "  inflating: indic-en/vocab/vocab.TGT  \n",
      "  inflating: indic-en/vocab/bpe_codes.32k.TGT  \n",
      "   creating: indic-en/final_bin/\n",
      "  inflating: indic-en/final_bin/dict.TGT.txt  \n",
      "  inflating: indic-en/final_bin/dict.SRC.txt  \n",
      "   creating: indic-en/model/\n",
      "  inflating: indic-en/model/checkpoint_best.pt  \n",
      "--2021-10-30 03:29:59--  https://storage.googleapis.com/samanantar-public/V0.2/models/en-indic.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.138.128, 108.177.122.128, 64.233.185.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.138.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4609212103 (4.3G) [application/zip]\n",
      "Saving to: ‘en-indic.zip’\n",
      "\n",
      "en-indic.zip        100%[===================>]   4.29G  26.6MB/s    in 2m 45s  \n",
      "\n",
      "2021-10-30 03:32:49 (26.7 MB/s) - ‘en-indic.zip’ saved [4609212103/4609212103]\n",
      "\n",
      "Archive:  en-indic.zip\n",
      "   creating: en-indic/\n",
      "   creating: en-indic/vocab/\n",
      "  inflating: en-indic/vocab/bpe_codes.32k.SRC  \n",
      "  inflating: en-indic/vocab/vocab.SRC  \n",
      "  inflating: en-indic/vocab/vocab.TGT  \n",
      "  inflating: en-indic/vocab/bpe_codes.32k.TGT  \n",
      "   creating: en-indic/final_bin/\n",
      "  inflating: en-indic/final_bin/dict.TGT.txt  \n",
      "  inflating: en-indic/final_bin/dict.SRC.txt  \n",
      "   creating: en-indic/model/\n",
      "  inflating: en-indic/model/checkpoint_best.pt  \n",
      "--2021-10-30 03:33:49--  https://storage.googleapis.com/samanantar-public/V0.3/models/m2m.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.122.128, 64.233.185.128, 64.233.177.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.122.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4081990185 (3.8G) [application/zip]\n",
      "Saving to: ‘m2m.zip’\n",
      "\n",
      "m2m.zip             100%[===================>]   3.80G  26.5MB/s    in 2m 27s  \n",
      "\n",
      "2021-10-30 03:36:21 (26.6 MB/s) - ‘m2m.zip’ saved [4081990185/4081990185]\n",
      "\n",
      "Archive:  m2m.zip\n",
      "   creating: m2m/\n",
      "   creating: m2m/vocab/\n",
      "  inflating: m2m/vocab/vocab.SRC     \n",
      "  inflating: m2m/vocab/vocab.TGT     \n",
      "  inflating: m2m/vocab/bpe_codes.32k.SRC_TGT  \n",
      "   creating: m2m/final_bin/\n",
      "  inflating: m2m/final_bin/dict.TGT.txt  \n",
      "  inflating: m2m/final_bin/dict.SRC.txt  \n",
      "   creating: m2m/model/\n",
      "  inflating: m2m/model/checkpoint_best.pt  \n",
      "/w266/project/bleu_test/testing/indicTrans\n"
     ]
    }
   ],
   "source": [
    "# download the indictrans model\n",
    "\n",
    "\n",
    "# downloading the indic-en model\n",
    "!wget https://storage.googleapis.com/samanantar-public/V0.2/models/indic-en.zip\n",
    "!unzip indic-en.zip\n",
    "\n",
    "# downloading the en-indic model\n",
    "!wget https://storage.googleapis.com/samanantar-public/V0.2/models/en-indic.zip\n",
    "!unzip en-indic.zip\n",
    "\n",
    "# downloading the indic-indic model\n",
    "!wget https://storage.googleapis.com/samanantar-public/V0.3/models/m2m.zip\n",
    "!unzip m2m.zip\n",
    "\n",
    "%cd indicTrans/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a text file and adding en sentences we can use for testing the model\n",
    "!touch en_sentences.txt\n",
    "!echo 'This bicycle is too small for you !!' >> en_sentences.txt\n",
    "!echo \"I will directly meet you at the airport.\" >> en_sentences.txt\n",
    "!echo 'If COVID-19 is spreading in your community, stay safe by taking some simple precautions, such as physical distancing, wearing a mask, keeping rooms well ventilated, avoiding crowds, cleaning your hands, and coughing into a bent elbow or tissue' >> en_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/w266/project/bleu_test/testing/indicTrans\n"
     ]
    }
   ],
   "source": [
    "%cd testing/indicTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 30 14:39:01 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 83.68it/s]\n",
      "Number of sentences in input: 3\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "# joint_translate takes src_file, output_fname, src_lang, tgt_lang, model_folder as inputs\n",
    "# src_file -> input text file to be translated\n",
    "# output_fname -> name of the output file (will get created) containing the model predictions\n",
    "# src_lang -> source lang code of the input text ( in this case we are using en-indic model and hence src_lang would be 'en')\n",
    "# tgt_lang -> target lang code of the input text ( tgt lang for en-indic model would be any of the 11 indic langs we trained on:\n",
    "#              as, bn, hi, gu, kn, ml, mr, or, pa, ta, te)\n",
    "# supported languages are:\n",
    "#              as - assamese, bn - bengali, gu - gujarathi, hi - hindi, kn - kannada, \n",
    "#              ml - malayalam, mr - marathi, or - oriya, pa - punjabi, ta - tamil, te - telugu\n",
    "\n",
    "# model_dir -> the directory containing the model and the vocab files\n",
    "\n",
    "# Note: if the translation is taking a lot of time, please tune the buffer_size and batch_size parameter for fairseq-interactive defined inside this joint_translate script\n",
    "\n",
    "\n",
    "# here we are translating the english sentences to tamil\n",
    "!bash joint_translate.sh en_sentences.txt ta_outputs.txt 'en' 'ta' '../en-indic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "இந்த சைக்கிள் உங்களுக்கு மிகவும் சிறியது!\n",
      "விமான நிலையத்தில் உங்களை நேரில் சந்திக்கிறேன்.\n",
      "உங்கள் சமூகத்தில் கோவிட்-19 பரவுகிறது என்றால், சில எளிய முன்னெச்சரிக்கை நடவடிக்கைகளான, தனி நபர் இடைவெளி, முகக்கவசம் அணிதல், அறைகளை நன்கு காற்றோட்டமாக வைத்திருத்தல், கூட்டத்தைத் தவிர்த்தல், கைகளைக் கழுவுதல், முழங்கை அல்லது திசுக்களில் இருமல் போன்றவற்றை மேற்கொள்வதன் மூலம் பாதுகாப்பாக இருங்கள்.\n"
     ]
    }
   ],
   "source": [
    "!cat ta_outputs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ta_outputs.txt ta_outputs1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta_outputs.txt\tta_outputs1.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ta*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ref_fname'\n",
      "Traceback (most recent call last):\n",
      "  File \"scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'pred_fname'\n",
      "compute_bleu.sh: line 27: pred_fname.tok: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "pred_fname = 'ta_outputs.txt'\n",
    "ref_fname  = 'ta_outputs1.txt'\n",
    "src_lang = 'ta'\n",
    "tgt_lang = 'ta'\n",
    "# to compute bleu scores for the predicitions with a reference file, use the following command\n",
    "\n",
    "!bash compute_bleu.sh pred_fname ref_fname src_lang tgt_lang\n",
    "# arguments:\n",
    "# pred_fname: file that contains model predictions\n",
    "# ref_fname: file that contains references\n",
    "# src_lang and tgt_lang : the source and target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a text file and adding hi sentences we can use for testing the model\n",
    "!touch hi_sent_new.txt\n",
    "!echo 'तुम आज सुबह यहाँ क्यों आए?' >> hi_sent_new.txt\n",
    "!echo \"मेरे परिवार में हर कोई जल्दी उठता है।\" >> hi_sent_new.txt\n",
    "!echo ' स्वास्थ्य और परिवार कल्याण मंत्रालय द्वारा प्रदान की गई जानकारी और सलाह को सावधानी व सही तरीके से पालन कर वायरस के स्थानीय प्रसार को रोका जा सकता है।' >> hi_sent_new.txt\n",
    "\n",
    "!touch ta_sent_new.txt\n",
    "!echo 'அவனுக்கு நம்மைப் தெரியும் என்று தோன்றுகிறது' >> ta_sent_new.txt\n",
    "!echo \"இது எங்கே இருக்கு என்று என்னால் கண்டுபிடிக்க முடியவில்லை.\" >> ta_sent_new.txt\n",
    "!echo 'உங்களுக்கு உங்கள் அருகில் இருக்கும் ஒருவருக்கோ இத்தகைய அறிகுறிகள் தென்பட்டால், வீட்டிலேயே இருப்பது, கொரோனா வைரஸ் தொற்று பிறருக்கு வராமல் தடுக்க உதவும்.' >> ta_sent_new.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 30 16:09:00 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 80.05it/s]\n",
      "Number of sentences in input: 3\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we are translating the english sentences to hindi\n",
    "!bash joint_translate.sh hi_sent_new.txt en_outputs_new.txt 'hi' 'en' '../indic-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 30 16:09:46 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 75.25it/s]\n",
      "Number of sentences in input: 3\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we are translating the english sentences to hindi\n",
    "!bash joint_translate.sh  en_outputs_new.txt hi_sent_new_dt.txt 'en' 'hi' '../en-indic' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 73.28it/s]\n",
      "3\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 64.75it/s]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "!python scripts/preprocess_translate.py hi_sent_new_dt.txt hi_sent_new_dt.txt.tok hi\n",
    "\n",
    "!python scripts/preprocess_translate.py hi_sent_new.txt hi_sent_new.txt.tok hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;39m63.9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sacrebleu --tokenize none hi_sent_new.txt.tok < hi_sent_new_dt.txt.tok | jq -r .score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "तुम आज सुबह यहाँ क्यों आए?\n",
      "मेरे परिवार में हर कोई जल्दी उठता है।\n",
      " स्वास्थ्य और परिवार कल्याण मंत्रालय द्वारा प्रदान की गई जानकारी और सलाह को सावधानी व सही तरीके से पालन कर वायरस के स्थानीय प्रसार को रोका जा सकता है।\n"
     ]
    }
   ],
   "source": [
    "! cat hi_sent_new.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आप सुबह यहां क्यों आए?\n",
      "मेरे परिवार में हर कोई जल्दी उठता है।\n",
      "स्वास्थ्य और परिवार कल्याण मंत्रालय द्वारा दी गई जानकारी और सलाह का सावधानीपूर्वक और सही तरीके से पालन करके वायरस के स्थानीय प्रसार को रोका जा सकता है।\n"
     ]
    }
   ],
   "source": [
    "!cat hi_sent_new_dt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local spread of the virus can be curbed by following the information and advice provided by the Ministry of Health and Family Welfare in a careful and correct manner.\n"
     ]
    }
   ],
   "source": [
    "! cat en_outputs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just rename the m2m_joint_vocab file here as joint_translate uses bpe_codes.32k.SRC\n",
    "!mv ../m2m/vocab/bpe_codes.32k.SRC_TGT ../m2m/vocab/bpe_codes.32k.SRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 30 14:56:16 UTC 2021\n",
      "Applying normalization and script conversion\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 46.86it/s]\n",
      "Number of sentences in input: 2\n",
      "Applying BPE\n",
      "Decoding\n",
      "Extracting translations, script conversion and detokenization\n",
      "Translation completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we are using the indic2indic model for translating the hindi sentences to tamil\n",
    "!bash joint_translate.sh hi_sent.txt ta_outp.txt 'hi' 'ta' '../m2m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ref_fname'\n",
      "Traceback (most recent call last):\n",
      "  File \"scripts/preprocess_translate.py\", line 172, in <module>\n",
      "    print(preprocess(infname, outfname, lang, transliterate))\n",
      "  File \"scripts/preprocess_translate.py\", line 61, in preprocess\n",
      "    num_lines = sum(1 for line in open(infname, \"r\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'pred_fname'\n",
      "compute_bleu.sh: line 27: pred_fname.tok: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "pred_fname = 'hi_sent.txt'\n",
    "ref_fname  = 'ta_outp.txt'\n",
    "src_lang = 'hi'\n",
    "tgt_lang = 'ta'\n",
    "# to compute bleu scores for the predicitions with a reference file, use the following command\n",
    "\n",
    "!bash compute_bleu.sh pred_fname ref_fname src_lang tgt_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 50.09it/s]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "!python scripts/preprocess_translate.py hi_sent.txt hi_sent.txt.tok hi\n",
    "\n",
    "!python scripts/preprocess_translate.py ta_outp.txt ta_outp.txt.tok ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 2.8,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0\",\n",
      " \"verbose_score\": \"6.2/3.6/2.1/1.2 (BP = 1.000 ratio = 1.333 hyp_len = 16 ref_len = 12)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"none\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.0.0\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!sacrebleu --tokenize none ta_outp.txt.tok < hi_sent.txt.tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libjq1 libonig4\n",
      "The following NEW packages will be installed:\n",
      "  jq libjq1 libonig4\n",
      "0 upgraded, 3 newly installed, 0 to remove and 5 not upgraded.\n",
      "Need to get 276 kB of archives.\n",
      "After this operation, 930 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libonig4 amd64 6.7.0-1 [119 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libjq1 amd64 1.5+dfsg-2 [111 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 jq amd64 1.5+dfsg-2 [45.6 kB]\n",
      "Fetched 276 kB in 1s (349 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libonig4:amd64.\n",
      "(Reading database ... 17369 files and directories currently installed.)\n",
      "Preparing to unpack .../libonig4_6.7.0-1_amd64.deb ...\n",
      "Unpacking libonig4:amd64 (6.7.0-1) ...\n",
      "Selecting previously unselected package libjq1:amd64.\n",
      "Preparing to unpack .../libjq1_1.5+dfsg-2_amd64.deb ...\n",
      "Unpacking libjq1:amd64 (1.5+dfsg-2) ...\n",
      "Selecting previously unselected package jq.\n",
      "Preparing to unpack .../jq_1.5+dfsg-2_amd64.deb ...\n",
      "Unpacking jq (1.5+dfsg-2) ...\n",
      "Setting up libonig4:amd64 (6.7.0-1) ...\n",
      "Setting up libjq1:amd64 (1.5+dfsg-2) ...\n",
      "Setting up jq (1.5+dfsg-2) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install --assume-yes  jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;39m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sacrebleu --tokenize none hi_sent.txt.tok < hi_sent.txt.tok | jq -r .score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
